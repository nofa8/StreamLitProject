{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)]\n",
    "        )\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    train_dir_1 = 'trainning/train1'\n",
    "    train_dir_2 = 'trainning/train2'\n",
    "    train_dir_3 = 'trainning/train3'\n",
    "    validation_dir = 'train4' # Validation\n",
    "    train_dir_5 = 'trainning/train5'\n",
    "    test_dir = 'test'\n",
    "\n",
    "    trainning = [train_dir_1, train_dir_2,train_dir_3,train_dir_5]\n",
    "\n",
    "    train_dir = train_dir_2\n",
    "    IMG_SIZE = 150  # InceptionResNetV2 requires 299x299 images\n",
    "\n",
    "    # image_dataset_from_directory with labels=\"inferred\" for \n",
    "    # getting the images in the subdirectories and translating the subdirectory as a class \n",
    "    # of type categorical\n",
    "    #train_dataset = image_dataset_from_directory(train_dir,image_size=(IMG_SIZE, IMG_SIZE),batch_size=32, labels=\"inferred\", label_mode=\"categorical\")\n",
    "    test_dataset = image_dataset_from_directory(test_dir,image_size=(IMG_SIZE, IMG_SIZE), labels=\"inferred\",label_mode=\"categorical\")\n",
    "    validation_dataset = image_dataset_from_directory(validation_dir,image_size=(IMG_SIZE, IMG_SIZE), labels=\"inferred\",label_mode=\"categorical\")\n",
    "\n",
    "    train_dataset = tf.data.Dataset\n",
    "\n",
    "    for i in trainning:\n",
    "        if i == trainning[0]:\n",
    "            train_dataset = image_dataset_from_directory(i, image_size=(IMG_SIZE, IMG_SIZE), labels=\"inferred\", label_mode=\"categorical\")\n",
    "            continue\n",
    "        train_dataset = train_dataset.concatenate( image_dataset_from_directory(i, image_size=(IMG_SIZE, IMG_SIZE), labels=\"inferred\", label_mode=\"categorical\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graph(history):\n",
    "    # Use the correct key names from the history object\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.applications.densenet import DenseNet121, preprocess_input\n",
    "NUM_CLASSES = 10\n",
    "# Define base model\n",
    "base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "base_model.trainable = False\n",
    "def get_features_and_labels(dataset):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for images, labels in dataset:\n",
    "        preprocessed_images = preprocess_input(images)\n",
    "        features = base_model.predict(preprocessed_images)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "with tf.device('/gpu:0'):\n",
    "    train_features, train_labels = get_features_and_labels(train_dataset)\n",
    "    val_features, val_labels = get_features_and_labels(validation_dataset)\n",
    "    test_features, test_labels = get_features_and_labels(test_dataset)\n",
    "\n",
    "    # Convert the features and labels into tf.data.Dataset\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_labels))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_features.npy', train_features)\n",
    "np.save('val_features.npy', val_features)\n",
    "np.save('test_features.npy', test_features)\n",
    "np.save('train_labels.npy', train_labels)\n",
    "np.save('val_labels.npy', val_labels)\n",
    "np.save('test_labels.npy', test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import load\n",
    "import numpy as np\n",
    "train_features = load('train_features.npy')\n",
    "val_features = load('val_features.npy')\n",
    "test_features = load('test_features.npy')\n",
    "train_labels = load('train_labels.npy')\n",
    "val_labels = load('val_labels.npy')\n",
    "test_labels = load('test_labels.npy')\n",
    "\n",
    "train_gen = DataGenerator(train_features, train_labels, 32)\n",
    "val_gen = DataGenerator(val_features, val_labels, 32)\n",
    "test_gen = DataGenerator(test_features, test_labels, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    def create_model(trial):\n",
    "        keras.backend.clear_session()\n",
    "        # Define the regularization and dropout values\n",
    "        reg = keras.regularizers.l2(0.005)\n",
    "        dropFinal = 0.5\n",
    "\n",
    "        # Add custom top layers\n",
    "        #inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "        #x = base_model(inputs, training=False)\n",
    "        inputs = keras.Input(shape=(4, 4, 1024))\n",
    "        x = layers.Flatten()(inputs)\n",
    "        x = layers.Dense(trial.suggest_int('dense_units', 256, 512, step=256), activation='relu', kernel_regularizer=reg)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropFinal)(x)\n",
    "        outputs = layers.Dense(10, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=trial.suggest_float('learning_rate_IN', 1e-3, 1e-2,log=True),\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.6,\n",
    "            staircase=True\n",
    "        )\n",
    "\n",
    "        optimizer_options = ['Adam', 'RMSprop']\n",
    "        optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "        if optimizer_selected == 'Adam':\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        else:\n",
    "            optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def objective(trial):\n",
    "        model = create_model(trial)\n",
    "        \n",
    "        # ModelCheckPoint - por cada trial\n",
    "        callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath=f'models_T/model_best_{trial.number}.h5',\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                verbose=0\n",
    "            ),# EarlyStopping -> beneficia os modelos e o tempo que o estudo demora\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0,\n",
    "                patience=4,\n",
    "                verbose=0,\n",
    "                mode='min',\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ]\n",
    "        # Batch Size pequeno por questões de memória\n",
    "        batch_size = trial.suggest_int('batch_size', 8, 16, step=8)\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=50,\n",
    "            validation_data=val_gen,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        #graph(history)\n",
    "        return val_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    # Criar estudo \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=5)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(f\"Value: {trial.value}\")\n",
    "    print(\"Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# Load do novo melhor modelo\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.load_model(\"models_T\\model_best_0.h5\")\n",
    "#model = create_model(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath='models_T/model_best_First.h5',\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                verbose=0\n",
    "            ),\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0,\n",
    "                patience=4,\n",
    "                verbose=1,\n",
    "                mode='min',\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    history = model.fit(\n",
    "            train_gen,\n",
    "            batch_size=16,\n",
    "            epochs=100,\n",
    "            validation_data=val_gen,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    graph(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loss, val_acc = model.evaluate(test_gen) #test_gen\n",
    "print('Test_acc:', val_acc,'\\nTest_loss:', val_loss)\n",
    "\n",
    "#keras.models.save_model(model,'models_T/model_best_First.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "\n",
    "#model = create_model(study.best_trial)\n",
    "base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.backend.clear_session()\n",
    "model = keras.models.load_model(\"models_T\\model_best_DA.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.applications.densenet import preprocess_input\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    def create_model1(trial):\n",
    "        keras.backend.clear_session()\n",
    "        # Define the regularization and dropout values\n",
    "        reg = keras.regularizers.l2(0.005)\n",
    "        dropFinal = 0.5\n",
    "        data_augmentation = keras.Sequential(\n",
    "            [\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.1),\n",
    "            layers.RandomZoom(0.2),\n",
    "            ]\n",
    "            )\n",
    "\n",
    "        # Add custom top layers\n",
    "        inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "        x = data_augmentation(inputs)\n",
    "        x = preprocess_input(x)\n",
    "        x = base_model(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(trial.suggest_int('dense_units', 256, 512, step=256), activation='relu', kernel_regularizer=reg)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropFinal)(x)\n",
    "        outputs = layers.Dense(10, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=trial.suggest_float('learning_rate_IN', 1e-3, 1e-2,log=True),\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.6,\n",
    "            staircase=True\n",
    "        )\n",
    "\n",
    "        optimizer_options = ['Adam', 'RMSprop']\n",
    "        optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "        if optimizer_selected == 'Adam':\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        else:\n",
    "            optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def objective1(trial):\n",
    "        model = create_model1(trial)\n",
    "        \n",
    "        callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath=f'models_T/model_best_DA_{trial.number}.h5',\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                verbose=0\n",
    "            ),\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0,\n",
    "                patience=4,\n",
    "                verbose=0,\n",
    "                mode='min',\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ]\n",
    "        batch_size = trial.suggest_int('batch_size', 8, 16, step=8)\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            epochs=20,\n",
    "            validation_data=validation_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        graph(history)\n",
    "        return val_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective1, n_trials=3)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(f\"Value: {trial.value}\")\n",
    "    print(\"Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# Load do novo melhor modelo\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.load_model(\"models_T\\model_best_DA_0.h5\")\n",
    "#model = create_model(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath='models_T/model_best_DAFirst_0.h5',\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                verbose=0\n",
    "            ),\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0,\n",
    "                patience=3,\n",
    "                verbose=1,\n",
    "                mode='min',\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    history = model.fit(\n",
    "            train_dataset,\n",
    "            batch_size=16,\n",
    "            epochs=20,\n",
    "            validation_data=validation_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    graph(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = model.get_layer(\"densenet121\")\n",
    "base_model.trainable = True\n",
    "print(model.layers)\n",
    "#agora, aquecimento\n",
    "for layer in base_model.layers[:-6]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath='models_T/model_best_DAFirst_0_FT.h5',\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                verbose=0\n",
    "            ),\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0,\n",
    "                patience=3,\n",
    "                verbose=1,\n",
    "                mode='min',\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    history = model.fit(\n",
    "            train_dataset,\n",
    "            batch_size=16,\n",
    "            epochs=20,\n",
    "            validation_data=validation_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    graph(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(test_gen) #test_gen\n",
    "print('Test_acc:', val_acc,'\\nTest_loss:', val_loss)\n",
    "\n",
    "#keras.models.save_model(model,'models_T/model_best_DAFirst.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
