{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory\n",
    "import tensorflow as tf\n",
    "train_dir_1 = 'trainning/train1'\n",
    "train_dir_2 = 'trainning/train2'\n",
    "train_dir_3 = 'trainning/train3'\n",
    "validation_dir = 'train4' # Validation\n",
    "train_dir_5 = 'trainning/train5'\n",
    "test_dir = 'test'\n",
    "\n",
    "trainning = [train_dir_1, train_dir_2,train_dir_3,train_dir_5]\n",
    "\n",
    "train_dir = train_dir_2\n",
    "IMG_SIZE = 32 # 32x32\n",
    "\n",
    "# image_dataset_from_directory with labels=\"inferred\" for \n",
    "# getting the images in the subdirectories and translating the subdirectory as a class \n",
    "# of type categorical\n",
    "#train_dataset = image_dataset_from_directory(train_dir,image_size=(IMG_SIZE, IMG_SIZE),batch_size=32, labels=\"inferred\", label_mode=\"categorical\")\n",
    "test_dataset = image_dataset_from_directory(test_dir,image_size=(IMG_SIZE, IMG_SIZE), labels=\"inferred\",label_mode=\"categorical\")\n",
    "validation_dataset = image_dataset_from_directory(validation_dir,image_size=(IMG_SIZE, IMG_SIZE), labels=\"inferred\",label_mode=\"categorical\")\n",
    "\n",
    "train_dataset = tf.data.Dataset\n",
    "\n",
    "for i in trainning:\n",
    "    if i == trainning[0]:\n",
    "        train_dataset = image_dataset_from_directory(i, image_size=(IMG_SIZE, IMG_SIZE), labels=\"inferred\", label_mode=\"categorical\")\n",
    "        continue\n",
    "    train_dataset = train_dataset.concatenate( image_dataset_from_directory(i, image_size=(IMG_SIZE, IMG_SIZE), labels=\"inferred\", label_mode=\"categorical\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graph(history):\n",
    "    # Use the correct key names from the history object\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for data_batch, labels_batch in train_dataset:\n",
    "  print('data batch shape:', data_batch.shape)\n",
    "  print('labels batch shape:', labels_batch.shape)\n",
    "  break\n",
    "for data_batch, _ in train_dataset.take(1):\n",
    "  for i in range(3):\n",
    "    plt.imshow(data_batch[i].numpy().astype(\"uint8\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# Para revêr ou treinar mais\n",
    "model_path = \"models_S/S_without_DA.h5\"\n",
    "model =  tf.keras.models.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    n_conv_layers = trial.suggest_int('n_conv_layers', 2, 3)\n",
    "    #making the model\n",
    "    reg = keras.regularizers.l2(0.005)\n",
    "    dropConv = 0.2\n",
    "    dropFinal = 0.5\n",
    "    inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = layers.Rescaling(1./255)(inputs)\n",
    "    for i in range(n_conv_layers):\n",
    "        n_filters = trial.suggest_int('n_filters1_{}'.format(i), 32, 128, step=32)\n",
    "        n_filters2 = trial.suggest_int('n_filters2_{}'.format(i), 32, 128, step=32)\n",
    "        n_ksize = trial.suggest_int('n_ksize1_{}'.format(i), 2, 3)\n",
    "        n_ksize1 = trial.suggest_int('n_ksize2_{}'.format(i), 2, 3)\n",
    "        x = layers.Conv2D(filters=n_filters, kernel_size=n_ksize,padding=\"same\", activation=\"relu\",kernel_regularizer=reg)(x)\n",
    "        x = layers.BatchNormalization(axis=-1)(x)\n",
    "        x = layers.Conv2D(filters=n_filters2, kernel_size=n_ksize1,padding=\"same\", activation=\"relu\",kernel_regularizer=reg)(x)\n",
    "        x = layers.BatchNormalization(axis=-1)(x)\n",
    "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "        x = layers.Dropout(dropConv)(x)\n",
    "\n",
    "        \n",
    "    #The flatten and classification process    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\", kernel_regularizer=reg)(x) #trial.suggest_int('dense_units', 512, 1024, step=512)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropFinal)(x)\n",
    "\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=trial.suggest_float('learning_rate_IN', 1e-3, 1e-2, log=True),\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.6)\n",
    "\n",
    "    # Suggest optimizer\n",
    "    optimizer_options = ['Adam', 'RMSprop']\n",
    "    optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "    if optimizer_selected == 'Adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f'models_S/model_best_{trial.number}.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=0\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=4,\n",
    "        verbose=0,\n",
    "        mode='min',\n",
    "        restore_best_weights=True,\n",
    "    )]\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128, step=32)\n",
    "\n",
    "    \n",
    "    history = model.fit(train_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=30,  \n",
    "                        validation_data=validation_dataset,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=0)  # não verboso\n",
    "    graph(history)\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=6)  # Ajustar número de trials para um estudo mais completo\n",
    "\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "model_path = \"models_S/S_without_DA_1.h5\"\n",
    "model =  tf.keras.models.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models_S/S_without_DA.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True,\n",
    ")]\n",
    "    \n",
    "    \n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,  \n",
    "    batch_size=128,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")  # # sim/não verboso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(test_dataset)\n",
    "print('val_acc:', val_acc,'\\nval_loss:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar por alguma razão extra\n",
    "keras.models.save_model(model,\"models_S\\S_without_DA.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    n_conv_layers = trial.suggest_int('n_conv_layers', 2, 3)\n",
    "    # Definição da augmentação da informação\n",
    "    reg = keras.regularizers.l2(0.005)\n",
    "    dropConv = 0.2\n",
    "    dropFinal = 0.5\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),])\n",
    "    #\n",
    "    # Criação do dito Modelo\n",
    "    inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    # Data augmentation\n",
    "    x = data_augmentation(inputs)\n",
    "    #\n",
    "    x = layers.Rescaling(1./255)(inputs)\n",
    "    for i in range(n_conv_layers):\n",
    "        n_filters = trial.suggest_int('n_filters1_{}'.format(i), 32, 128, step=32)\n",
    "        #n_filters2 = trial.suggest_int('n_filters2_{}'.format(i), 32, 128, step=32)\n",
    "        n_filters2 = n_filters \n",
    "        n_ksize = trial.suggest_int('n_ksize1_{}'.format(i), 2, 3)\n",
    "        #n_ksize1 = trial.suggest_int('n_ksize2_{}'.format(i), 2, 3)\n",
    "        n_ksize1 = n_ksize\n",
    "        x = layers.Conv2D(filters=n_filters, kernel_size=n_ksize,padding=\"same\", activation=\"relu\",kernel_regularizer=reg)(x)\n",
    "        x = layers.BatchNormalization(axis=-1)(x)\n",
    "        x = layers.Conv2D(filters=n_filters2, kernel_size=n_ksize1,padding=\"same\", activation=\"relu\",kernel_regularizer=reg)(x)\n",
    "        x = layers.BatchNormalization(axis=-1)(x)\n",
    "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "        x = layers.Dropout(dropConv)(x)\n",
    "\n",
    "        \n",
    "    #The flatten and classification process    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(trial.suggest_int('dense_units', 256, 512, step=256), activation=\"relu\", kernel_regularizer=reg)(x) \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropFinal)(x)\n",
    "\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=trial.suggest_float('learning_rate_IN', 1e-3, 1e-2, log=True),\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.6)\n",
    "\n",
    "    # Suggest optimizer\n",
    "    optimizer_options = ['Adam', 'RMSprop']\n",
    "    optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "    if optimizer_selected == 'Adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f'models_S/model_best_DA_{trial.number}.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=0\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=4,\n",
    "        verbose=1,\n",
    "        mode='min',\n",
    "        restore_best_weights=True,\n",
    "    )]\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128, step=32)\n",
    "\n",
    "    \n",
    "    history = model.fit(train_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=50,  \n",
    "                        validation_data=validation_dataset,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=0)  # não verboso\n",
    "    graph(history)\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=2)   # Ajustar número de trials para um estudo mais completo\n",
    "\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# Load do novo melhor modelo\n",
    "best_model = keras.models.load_model(\"models_S\\model_best_DA_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models_S/S_with_DA.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=0\n",
    "    ), keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=2,\n",
    "        verbose=0,\n",
    "        mode='min',\n",
    "        restore_best_weights=True,\n",
    "    )]\n",
    "    \n",
    "    \n",
    "history = best_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100, batch_size=128, \n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = best_model.evaluate(validation_dataset)\n",
    "print('val_acc:', val_acc,'\\nval_loss:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(best_model,\"models_S\\S_with_DA.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
